\documentclass{article} % For LaTeX2e
\usepackage[backend=bibtex]{biblatex}
\addbibresource{sample.bib}
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{tipa}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[linesnumberedhidden,ruled,noend]{algorithm2e}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{COMPM083: Statistical Natural Language Processing - Assignment 2}

\author{
Sergiu Tripon\\
Department of Computer Science\\
University College London\\
Gower Street, London, WC1E\\
\texttt{sergiu.tripon.15@ucl.ac.uk}\\
\And
Santiago Gonzalez\\
Department of Computer Science\\
University College London\\
Gower Street, London, WC1E\\
\texttt{hernan.toral.15@ucl.ac.uk} \\
\AND
Archie Norman \\
Department of Computer Science\\
University College London\\
Gower Street, London, WC1E\\
\texttt{archie.norman.15@ucl.ac.uk} \\
\And
Eduardo Litonjua\\
Department of Computer Science\\
University College London\\
Gower Street, London, WC1E\\
\texttt{eduardo.litonjua.14@ucl.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\section*{Problem 1: Implementation of Perceptron Algorithm - Evaluation Results}

\begin{table}[!htbp]
\caption{Similarity on Average and Per-class evaluation comparison for Trigger extraction.}
\label{table:Perceptron}
\begin{center}
\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf My Trainer}  &\multicolumn{3}{c}{\bf Precompiled Trainer}
\\ \hline \\
{} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} Score & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
Average             & 0.526 & 0.665 & 0.587 & 0.526 & 0.665 & 0.587\\
Phosphorylation     & 0.75  & 1.0   & 0.857 & 0.75  & 1.0   & 0.857\\
Negative Regulation & 0.690 & 0.603 & 0.644 & 0.690 & 0.603 & 0.644\\
Regulation          & 0.422 & 0.678 & 0.520 & 0.422 & 0.678 & 0.520\\
Binding             & 0.016 & 1.0   & 0.031 & 0.016 & 1.0   & 0.031\\
Positive Regulation & 0.408 & 0.64  & 0.498 & 0.408 & 0.64  & 0.498\\
Localization        & 0.0   & 0.0   & 0.0   & 0.0   & 0.0   & 0.0\\
Transcription       & 0.0   & 0.0   & 0.0   & 0.0   & 0.0   & 0.0\\
None                & 0.914 & 0.849 & 0.880 & 0.914 & 0.849 & 0.880\\
Gene Expression     & 0.881 & 0.761 & 0.817 & 0.881 & 0.761 & 0.817\\
\end{tabular}
\end{center}
\end{table}

\section*{Problem 2: Implementation of Average Perceptron}
The Perceptron algorithm returns the most recent version of the weight vector, whereas the Average Perceptron algorithm computes and returns the average of all instances of the weight vector\cite{collins2002discriminative}.

The Average Perceptron algorithm can be implemented naively, this would indicate that the algorithm would be slow at run time. It maintains an average weight vector which is updated for every instance regardless of whether it was correctly classified. Maintaining and updating each version of the weight vector on every iteration is impractical and inefficient.

In order to improve the naive implementation of the Average Perceptron algorithm, we propose a solution whereby the average weight vector is not updated for every instance, but only for instances that were incorrectly classified. We do this as only incorrect classifications have an affect on vector weight changes. In addition, We can improve computational speed by calculating the average sum of all updates once over every instance. Table \ref{table:AveragePerceptron} shows that we get the exact same values as the provided precompiled algorithm.

%\begin{enumerate}
%\item Introduced steps variable which is the result of iterations multiplied by number of instances in the training set
%\item The learning rate within the average weight vector is multiplied by the steps variable
%\item The steps variable is used to move to the next iteration step
%\item Before returning the average weight vector, its values are divided by the steps variable
%\end{enumerate}

\begin{table}[!htbp]
\caption{Average and Per-class evaluation comparison for Trigger extraction.}
\label{table:AveragePerceptron}
\begin{center}
\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf My Trainer}  &\multicolumn{3}{c}{\bf Precompiled Trainer}
\\ \hline \\
{} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} Score & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
Average             & 0.210 & 0.590 & 0.310 & 0.210 & 0.590 & 0.310\\
Phosphorylation     & 0.000   & 0.000   & 0.000   & 0.000   & 0.000   & 0.000\\
Negative Regulation & 0.679 & 0.571 & 0.620 & 0.679 & 0.571 & 0.620\\
Regulation          & 0.444 & 0.571 & 0.500   & 0.444 & 0.571 & 0.500\\
Binding             & 0.031 & 1.000   & 0.060 & 0.031 & 1.000   & 0.060\\
Positive Regulation & 0.136 & 0.920  & 0.238 & 0.136 & 0.920  & 0.238\\
Localization        & 0.000   & 0.000   & 0.000   & 0.000   & 0.000   & 0.000\\
Transcription       & 0.235 & 0.307 & 0.266 & 0.235 & 0.307 & 0.266\\
None                & 0.984 & 0.512 & 0.673 & 0.984 & 0.512 & 0.673\\
Gene Expression     & 0.787 & 0.357 & 0.492 & 0.780  & 0.350  & 0.490\\
\end{tabular}
\end{center}
\end{table}

\section*{Problem 3: Feature Engineering and Evaluation \cite{mcclosky2011event}\cite{miwa2010event}\cite{riedel2011model}} 

We develop two different sets of features for trigger and argument extraction tasks, and evaluate the results of running two classification models with the best features to maximize its predictions. Table \ref{table:Parameters} reports the parameters used for running the algorithms.

\begin{table}[!htbp]
\caption{Parameters for Document extraction and Algorithm Setup}
\label{table:Parameters}
\begin{center}
\begin{tabular}{c c c c c}
\multicolumn{3}{c}{\bf Training/Sampling}  &\multicolumn{2}{c}{\bf Perceptron Model}
\\ \hline \\
Train Docs & Training Ratio & Trigger/Arg Subsampling & No. of Iterations & Learning Rate\\
1000 & 80\% & 0.02/0.008 & 10 & 1.0\\
\end{tabular}
\end{center}
\end{table}

%\subsection*{Naive Bayes (MLE) Algorithm}
The average evaluation results of the extraction task for the \textit{Naive Bayes (MLE) Algorithm} are presented on Table \ref{table:EvalNB}, while Table \ref{table:NBTriggerFeat} and Table \ref{table:NBArgFeat} shows the performance of each of the features in the training.

\begin{table}[!hbp]
\caption{Evaluation results of Naive Bayes algorithm for Trigger and Argument Extraction (average).}
\begin{center}
\label{table:EvalNB}

\begin{tabular}{c c c c c c}
\multicolumn{3}{c}{\bf Trigger Extraction}  &\multicolumn{3}{c}{\bf Argument Extraction}
\\ \hline \\
\textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
0.201 & 0.833 & 0.324 & 0.056 & 0.858 & 0.106\\
\end{tabular}
\end{center}
\end{table}

%========================

\begin{table}[!hbp]
\begin{center}

\caption{Best set of Trigger Features for the Naive Bayes algorithm, and their impact on weight.}
\label{table:NBTriggerFeat}

\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Before (no features)}  &\multicolumn{3}{c}{\bf After (with feature)}
\\ \hline \\
\textbf{Feature} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
label bias & 0.002 & 0.020 & 0.003 & 0.033 & 0.339 & 0.060\\
first trigger word & 0.002 & 0.020 & 0.003 & 0.170 & 0.847 & 0.284\\
trigger dictionary starts with & 0.002 & 0.020 & 0.033 & 0.001 & 0.019 & 0.003\\
first mention length & 0.002 & 0.020 & 0.033 & 0.013 & 0.093 & 0.024\\
\end{tabular}
\end{center}
\end{table}

%========================

\begin{table}[!htbp]
\caption{Best set of Argument Features for the Naive Bayes Algorithm, and their impact on weight.}
\label{table:NBArgFeat}
\begin{center}
\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Before (no features)}  &\multicolumn{3}{c}{\bf After (with feature)}
\\ \hline \\
\textbf{Feature} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
label bias             & 0.010 & 0.880 & 0.020 & 0.010 & 0.880 & 0.020\\
is protein\textunderscore first trigger word & 0.010 & 0.880 & 0.020 & 0.034 & 0.743 & 0.065\\
first argument pos & 0.010 & 0.880 & 0.020 & 0.022 & 0.831 & 0.043\\
first token of event pos & 0.010 & 0.880 & 0.020 & 0.012 & 0.486 & 0.024\\
first mention length   & 0.010 & 0.880 & 0.020 & 0.008 & 0.341 & 0.015\\
%dist. between event and event.begin & 0.010 & 0.880 & 0.020 & 0.013 & 0.582 & 0.026\\
%dep head/mod to protein & 0.010 & 0.880 & 0.020 & 0.016 & 0.672 & 0.032\\
\end{tabular}
\end{center}
\end{table}

%====================

\subsection*{Perceptron Algorithm}
Evaluation results for the \textit{Perceptron Algorithm} are presented on Table \ref{table:EvalPerc}, whilst Table \ref{table:PercTriggerFeat} and Table \ref{table:PercArgFeat} show the performance of each of the features in the training.


\begin{table}[!hbp]
\caption{Evaluation results of Perceptron algorithm for Trigger and Argument Extraction (average).}
\begin{center}
\label{table:EvalPerc}
\begin{tabular}{c c c c c c}
\multicolumn{3}{c}{\bf Trigger Extraction}  &\multicolumn{3}{c}{\bf Argument Extraction}
\\ \hline \\
\textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
0.305 & 0.777 & 0.438 & 0.121 & 0.837 & 0.212\\
\end{tabular}
\end{center}
\end{table}

\begin{table}[!hbp]
\caption{Best set of Trigger Features for the Perceptron Algorithm, and their impact on weight.}
\label{table:PercTriggerFeat}
\begin{center}
\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Before (no features)}  &\multicolumn{3}{c}{\bf After (with feature)}
\\ \hline \\
\textbf{Feature} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
label bias & 0.002 & 0.020 & 0.003 & 0.019 & 0.201 & 0.035\\
first trigger word/pos/stem & 0.002 & 0.020 & 0.003 & 0.176 & 0.812 & 0.290\\
first trigger length & 0.002 & 0.020 & 0.003 & 0.063 & 0.383 & 0.109\\

left/right pos on nouns/verbs & 0.002 & 0.020 & 0.003 & 0.002 & 0.025 & 0.004\\

\iffalse
left/right pos & \multirow{2}{*}{0.002} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.003} & \multirow{2}{*}{0.002} & \multirow{2}{*}{0.025} & \multirow{2}{*}{0.004}\\
on nouns/verbs\\
\fi

none trigger event & 0.002 & 0.020 & 0.003 & 0.002 & 0.020 & 0.004\\
none trigger event to & 0.002 & 0.020 & 0.003 & 0.002 & 0.022 & 0.004\\

token bigram on the left/right & 0.002 & 0.020 & 0.003 & 0.084 & 0.610 & 0.148\\

\iffalse
token bigram on & \multirow{2}{*}{0.002} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.003} & \multirow{2}{*}{0.084} & \multirow{2}{*}{0.610} & \multirow{2}{*}{0.148}\\
the left/right\\
\fi

token starts with/contains "-" & 0.002 & 0.020 & 0.003 & 0.004 & 0.041 & 0.007\\

\iffalse
token starts & \multirow{2}{*}{0.002} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.003} & \multirow{2}{*}{0.004} & \multirow{2}{*}{0.041} & \multirow{2}{*}{0.007}\\
with/contains "-"\\
\fi

trigger dictionary starts/ends with & 0.002 & 0.020 & 0.003 & 0.021 & 0.220 & 0.039\\

\iffalse
trigger dictionary & \multirow{2}{*}{0.002} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.003} & \multirow{2}{*}{0.021} & \multirow{2}{*}{0.220} & \multirow{2}{*}{0.039}\\
starts/ends with\\
\fi

first mention length   & 0.002 & 0.020 & 0.003 & 0.024 & 0.164 & 0.042\\
distance to left/right mention & 0.002 & 0.020 & 0.003 & 0.025 & 0.242 & 0.045\\
dep head/mod & 0.002 & 0.020 & 0.003 & 0.029 & 0.269 & 0.053\\
\end{tabular}
\end{center}
\end{table}

\begin{table}[!htp]
\caption{Best set of Argument Features for the Perceptron Algorithm, and their impact on weight.}
\label{table:PercArgFeat}
\begin{center}
\begin{tabular}{c c c c c c c }
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Before (no features)}  &\multicolumn{3}{c}{\bf After (with feature)}
\\ \hline \\
\textbf{Feature} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
\hline
label bias & 0.010 & 0.880 & 0.020 & 0.010 & 0.880 & 0.020\\
\hline
%first argument word/pos/stem & 0.010 & 0.880 & 0.020 & 0.028 & 0.774 & 0.054\\

first argument & \multirow{2}{*}{0.010} & \multirow{2}{*}{0.880} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.028} & \multirow{2}{*}{0.774} & \multirow{2}{*}{0.054}\\
word/pos/stem\\
\hline
is protein\textunderscore first trigger word & 0.010 & 0.880 & 0.020 & 0.041 & 0.786 & 0.079\\
\hline
first token of event pos & 0.010 & 0.880 & 0.020 & 0.017 & 0.641 & 0.033\\
\hline
is protein\textunderscore first & 0.010 & 0.880 & 0.020 & 0.010 & 0.880 & 0.020\\
\hline
%unigram/bigram on the left/right using begin & 0.010 & 0.880 & 0.020 & 0.028 & 0.790 & 0.055\\

unigram/bigram on the & \multirow{2}{*}{0.010} & \multirow{2}{*}{0.880} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.028} & \multirow{2}{*}{0.790} & \multirow{2}{*}{0.055}\\
left/right using begin\\
\hline
%unigram/bigram on the left/right using event.begin & 0.010 & 0.880 & 0.020 & 0.021 & 0.648 & 0.040\\

unigram/bigram on the & \multirow{2}{*}{0.010} & \multirow{2}{*}{0.880} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.021} & \multirow{2}{*}{0.648} & \multirow{2}{*}{0.040}\\
left/right using event.begin\\
\hline
%distance between event and event.begin & 0.010 & 0.880 & 0.020 & 0.013 & 0.722 & 0.027\\

distance between event & \multirow{2}{*}{0.010} & \multirow{2}{*}{0.880} & \multirow{2}{*}{0.020} & \multirow{2}{*}{0.013} & \multirow{2}{*}{0.722} & \multirow{2}{*}{0.027}\\
and event.begin\\
\hline
%none coming/going arguments due to no dependencies & 0.010 & 0.880 & 0.020 & 0.010 & 0.880 & 0.020\\

none coming/going & \multirow{3}{*}{0.010} & \multirow{3}{*}{0.880} & \multirow{3}{*}{0.020} & \multirow{3}{*}{0.010} & \multirow{3}{*}{0.880} & \multirow{3}{*}{0.020}\\
arguments due to\\
no dependencies\\
\hline
dep head/mod & 0.010 & 0.880 & 0.020 & 0.014 & 0.770 & 0.027\\
\end{tabular}
\end{center}
\end{table}

\subsection*{Features choice justification}

\textbf{Trigger Features}

\begin{enumerate}

\item \textbf{label bias} - A default feature for the bias term for non representative instances.

\item \textbf{first trigger word} - Feature for most common words as trigger candidates.

\item \textbf{first trigger pos/stem} - Observing the first trigger word feature provided, a feature template was created to evaluate the first candidate’s part of speech and stem to the trigger label in order to find out if it learns specific patterns within the data. Moreover, the feature improves recall.

\item \textbf{first trigger length} - The feature evaluates the length of common words to improve the score in altogether with other word-based features. It has been found that it improves recall.
%This feature calculates the number candidates in the sentence and evaluates those candidates to the trigger label. This feature helps to evaluate more candidates at the same time instead of just the first one, and it has been found that it improves recall.

\item \textbf{none trigger event} - The feature helps to classify None triggers based on the POS part of them. A common pattern of POS on candidate and direct neighbors where found during the data analysis task, and the proposed feature improved the precision of the results.

\item \textbf{none trigger event to} - Similar to the previous one, the feature helps to minimize the misclassification of None triggers by modeling a common syntactic pattern of verbs and preposition.

\item \textbf{left/right pos on nouns/ verbs} - Precision is improved by taking care of common patterns where the POS of not None trigger candidates is whether Noun or Verb, and including the patterns of its direct neighbors as well.

\item \textbf{token bigram on the left/right} - This feature template is a one step further from the first trigger word feature template provided. It Evaluates a bigram of the current token and its left or right neighbours, helping to improves the learning process as it provides more knowledge about what’s before and after the current token. Therefore, the prediction of whether the current token is a trigger label or not becomes more precise.

\item \textbf {trigger starts with/contains "-"} - 
As a result of performing extensive data analysis on the data provided,a number of patterns have emerged:
\begin{enumerate}
\item High probability of tokens starting with “-” to be Trigger events.
\item High probability of tokens containing “-” to be proteins (mentions).
\end{enumerate}

\item \textbf{trigger dictionary starts/ends with} - The feature finds common patterns between trigger label and candidate word segments of words containing "-".

\item \textbf{first mention length} - As mentions (proteins) can also be triggers, a feature template was created in order to find out how many mentions are in one sentence as well as evaluate those mentions to the trigger label. A high number of mentions in a sentence, would mean a high probability that those mentions are in fact trigger labels. Adding this feature to the feature set, improved the prediction’s precision significantly.

\item \textbf{distance to left/right mention} - As mentions (proteins) can also be triggers, a feature template was created in order to evaluate the probability of a mention being a trigger based on the distance to the left and right mention. This calculation has improved precision dramatically.

\item \textbf{dep head/mod} - Similarly to the feature template first trigger word (feature 2), a feature template based on dependencies was created in order to evaluate the dependency labels to the trigger labels and observe whether there are any similarities and how the model learning process behaves. Based on the result, the prediction’s precision increased.
\end{enumerate}

\textbf{Argument Features}\

\begin{enumerate}

\item \textbf{label bias} - A default feature to control the zero predicted candidates.

\item \textbf{first mention length} - Feature used for both Trigger and Argument Extraction. See justification in the Trigger Features section above.

\item \textbf{first argument word/pos/stem} - Having access to the first argument, enabled the ability to create a feature which evaluates the first argument word, pos and stem in order to find out if there is a match and the first argument word is a valid argument.

\item \textbf{is protein\textunderscore} - By identifying whether the argument head word is a protein, the chances of that word to be involved in an argument extraction increase, as proteins have a high probability of both being triggers and begin involved in an argument.

\item \textbf{first token of event pos} - This feature is similar to the trigger feature first trigger pos/stem and the justification for it would be the same. This feature slightly improves precision.

\item \textbf{is protein\textunderscore first trigger word} - The provided feature evaluates a bigram to the argument label and improves precision slightly.

\item \textbf{unigram/bigram on the left/right using begin/event.begin} - Similarly to the token bigram on the left/right trigger feature, this feature evaluates two aspects:
\begin{enumerate}
\item \textbf{unigrams} - the token to the left or the right of the first token of event
\item \textbf{bigrams}  - the token to the left or the right of the first token of event and the first token of event
\end{enumerate}

Using the unigrams and bigrams feature, the model learn the left and right neighbours of the head token of the event, which improves its prediction of the source and destination triggers which are related to one another through an argument. Using these features has a significant impact on the evaluation results, especially on precision.

\item \textbf{distance between event and event.begin} - By applying this feature, the distance between the start of the candidate and the start of the parent event is calculated for evaluation.

\item \textbf{none coming/going arguments due to no dependencies} - The feature evaluates the total number of associated syntactic dependencies in head and modifier words of the candidate. It allow us to model None events where no dependencies are found in context.

\item \textbf{dep head/mod} - Feature used for both Trigger and Argument Extraction. See justification in the Trigger Featuers section above.

\end{enumerate}

\section*{Problem 4: Joint Perceptron\cite{riedel2011robust}}

For a given token \textbf{x} and candidate argument tokens $c_1 ... c_m$, let \textbf{e} $\in \tau \rightarrow (\tau \cup None)$  be the label of the trigger, and \textbf{a} = $a_{c_1},...,a_{c_m}: a_{c_j} \in \mathcal{R} \rightarrow (\mathcal{R} \cup None)$ the labels of the argument candidates. Then the fully factorized model scores the joint model of \textbf{e} and \textbf{a} by using formula \ref{eq:jointPerceptron}:

\begin{equation}
\label{eq:jointPerceptron}
\textbf{s}(\textbf{e}, \textbf{a}; \textbf{x}, \boldsymbol{\lambda}) = \textbf{s}_\tau(\textbf{e}; \textbf{x}, \boldsymbol{\lambda}) + \sum_{j}\textbf{s}_R(\textbf{a}_{c_j}; \textbf{x}, \boldsymbol{\lambda})
\end{equation}

\subsection*{Unconstrained Joint Model}

Algorithm \ref{alg:unconstrained} shows the pseudocode for computing the an unconstrained joint model. Starting with a candidate token \textbf{x}, learning weight vector \textbf{$\lambda$} and feature vectors $\boldsymbol{\phi}_t, \boldsymbol{\phi}_c$, the \textit{argmax} method gets the maximum score for trigger label and the associated arguments labels in isolation, and as a dual decomposed manner. Finally, it returns the best joint scores for trigger and arguments on candidate token \textbf{x}.

\SetKwInput{KwInput}{Input}

\newlength\algowd
\def\savewd#1{\setbox0=\hbox{#1\hspace{.7in}}\algowd=\wd0\relax#1}
\newcommand\algolines[2]{\savewd{#1}%
  \tcp*{\parbox[t]{\dimexpr\algowidth-\algowd}{#2}}}

\begin{center}
%\begin{minipage}[t]{6.9cm}
%\null
 \begin{algorithm}[!htbp]
    \SetAlgoLined\DontPrintSemicolon
    \caption{Joint Unconstrained Model}
    \label{alg:unconstrained}

    \KwInput{$\textbf{x}$: Candidate $\boldsymbol{\lambda}$: weights\; $\boldsymbol{\phi}_t$: Trigger Features $\boldsymbol{\phi}_c$: Argument Features}
    \KwResult{$\textbf{s}(\textbf{e}, \textbf{a}; \textbf{x}, \boldsymbol{\lambda})$}
    %$\textbf{e} \leftarrow trigger\_candidate(\textbf{x})$\;
    
    $\boldsymbol{\rho}_\lambda(\textbf{x}_t|$\textbf{$\tau$})$ \leftarrow \langle\boldsymbol{\phi}_t($\textbf{$\tau$}$, \textbf{x}_t), \boldsymbol{\lambda}\rangle$\;
    
    $\widehat{\textbf{e}} \leftarrow argmax_e\boldsymbol{\rho}_\lambda(\textbf{x}_t|$\textbf{$\tau$}$)$\;
    
    $\textbf{a}_c \leftarrow argument\_candidates(\textbf{x})$\;
    \ForEach{$\textbf{a}_{c_j}$ in $\textbf{a}_c$}{
        $\boldsymbol{\rho}_\lambda(\textbf{a}_{c_j}|\mathcal{R}) \leftarrow \langle\boldsymbol{\phi}_c(\mathcal{R}, \textbf{a}_{c_j}), \boldsymbol{\lambda}\rangle$\;
        
        $\widehat{\textbf{a}_{c_j}} \leftarrow argmax_a\boldsymbol{\rho}_\lambda(\textbf{a}_{c_j}, \mathcal{R})$\;
    }
    
    \KwRet $\textbf{s}_\tau(\textbf{e}; \textbf{x}, \boldsymbol{\lambda}) + \sum_{j}\textbf{s}_R(\textbf{a}_{c_j}; \textbf{x}, \boldsymbol{\lambda})$\tcc*[f]{returns best $\widehat{\textbf{e}}$ and $\widehat{\textbf{a}_{c_j}}$ vector}\;
  \end{algorithm}
%\end{minipage}
\end{center}

\subsection*{Constrained Joint Model}

For the Constrained Joint Model, we add up three constraits that are evaluated to get the \textit{argmax} value for arguments. Inside Algorithm \ref{alg:constrained}, the method \textit{argmaxFcn} is responsible to compute the predictions for the candidate arguments.

\begin{center}
%\begin{minipage}[t]{9cm}
%\null
    \begin{algorithm}[!htp]
        \SetAlgoLined\DontPrintSemicolon
        \caption{Joint Constrained Model}
        \label{alg:constrained}
        \KwInput{$\textbf{x}$: Candidate $\boldsymbol{\lambda}$: weights\; $\boldsymbol{\phi}_t$: Trigger Features $\boldsymbol{\phi}_c$: Argument Features}
        \KwResult{$\textbf{s}(\textbf{e}, \textbf{a}_c; \textbf{x}, \boldsymbol{\lambda})$}

        initialization:\;
        %$count_{\textsc{theme}} \leftarrow 0, count_{\textsc{cause}} \leftarrow 0,$\; 
        $argmax_{score} \leftarrow -\infty, argmax_{position} \leftarrow 0$\;
        
        \SetKwFunction{argmaxFcn}{argmaxFcn}
        \SetKwProg{myalg}{Method}{}{}
        
        \myalg{\argmaxFcn{$\mathcal{R}, \widehat{\textbf{e}}, \textbf{a}_{c}, \boldsymbol{\lambda}, \boldsymbol{\phi}_c$}}{
            $\widehat{{a}_c} \leftarrow [\hspace{1pt}]$\;
            \ForEach{$\textbf{a}_{c_j}$ in $\textbf{a}_c$}{
                $\boldsymbol{\rho}_\lambda(\textbf{a}_{c_j}|\mathcal{R}) \leftarrow \langle\boldsymbol{\phi}_c(\mathcal{R}, \textbf{a}_{c_j}), \boldsymbol{\lambda}\rangle$\;
        
                $\widehat{\textbf{a}_{c_j}} \leftarrow argmax_a\boldsymbol{\rho}_\lambda(\textbf{a}_{c_j}, \mathcal{R})$\;
                %\Switch{value of $\widehat{\textbf{a}_{c_j}}$}{
                %    \lCase{"Theme"}{$count_{\textsc{theme}}++$}
                %    \lCase{"Cause"}{$count_{\textsc{cause}}++$}
                %    \lOther{break}
                %}
                \If{$\widehat{\textbf{a}_{c_j}^{score}} > argmax_{score}$}{
                    $argmax_{score} \leftarrow \textbf{a}_{c_j}^{score}$\;
                    $argmax_{position} \leftarrow j$\;
                }
            }
            
            \If(\tcc*[f]{None events cannot have arguments}){isNoneEvent($\widehat{\textbf{e}}$)}{
                \ForEach{$best_{a_{c_j}}$ in $\textbf{best}_{a_c}$}{
                    $\textbf{best}_{a_{c_j}} = None$\;
                }
            }
            \uElseIf(\tcc*[h]{Only regulation event have Cause args}){Not(isRegulation($\widehat{\textbf{e}}$))}{
                \ForEach{$best_{a_{c_j}}$ in $\textbf{best}_{a_c}$}{
                    $best_{a_{c_j}} = $ \leIf{isCause($best_{a_{c_j}}$)}{None}{$best_{a_{c_j}}$}
                }
            }
            \uElseIf(\tcc*[f]{events != None can have 1+ Theme}){Not(hasTheme($\textbf{best}_{a_c}$))}{
                $best_{a_{c_{{argmax_{position}}}}} = Theme$
            }
            
            \KwRet $\widehat{\textbf{a}_c}$\;
        }{}
        
        %$\textbf{e} \leftarrow trigger\_candidate(\textbf{x})$\;
        
        $\boldsymbol{\rho}_\lambda(\textbf{x}_t|$\textbf{$\tau$})$ \leftarrow \langle\boldsymbol{\phi}_t($\textbf{$\tau$}$, \textbf{x}_t), \boldsymbol{\lambda}\rangle$\;
    
        $\widehat{\textbf{e}} \leftarrow argmax_e\boldsymbol{\rho}_\lambda(\textbf{x}_t|$\textbf{$\tau$}$)$\;
    
        
        $\textbf{a}_c \leftarrow argument\_candidates(\textbf{x})$\;
        
        $\textbf{best}_{a_c} \leftarrow$ \argmaxFcn{$\mathcal{R}, \widehat{\textbf{e}}, \textbf{a}_c, \boldsymbol{\lambda}, \boldsymbol{\phi}_c$}\;
        
        \KwRet $\textbf{s}_\tau(\textbf{e}; \textbf{x}, \boldsymbol{\lambda}) + \sum_{j}\textbf{s}_R(\textbf{a}_{c_j}; \textbf{x}, \boldsymbol{\lambda})$\tcc*[f]{returns best $\widehat{\textbf{e}}$ and $\textbf{best}_{a_c}$ vector}\;
        
    
    \end{algorithm}
%\end{minipage}
\end{center}

\section*{Problem 5: Implementation and Evaluation for Problem 4}

The best set of trigger and argument features have been applied to the four algorithms discussed: Naive Bayes, Perceptron, Joint Unconstrained and Joint Constrained.

After evaluating the results of running the two joint algorithms and the previous Naive Bayes and Perceptron models using the best set of developed (trigger and argument) features, we can say that there is not one algorithm that scored the highest in both trigger and argument extraction tasks. Table \ref{table:AllAlgorithms} presents the evaluation results for all the algorithms using the same configuration as proposed in Problem 3. The Perceptron Algorithm obtained a better prediction for Trigger extraction, while the Joint Unconstrained model predicted better in the Argument extraction task.

\begin{table}[!htp]
\caption{Evaluation results comparison of the Naive Bayes, Perceptron, Joint Unconstrained and Joint Constrained algorithms.}
\label{table:AllAlgorithms}
\begin{center}
\begin{tabular}{c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Trigger Extraction} &\multicolumn{3}{c}{\bf Argument Extraction}
\\ \hline \\
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score}\\
Naive Bayes         & 0.201 & 0.833 & 0.324 & 0.056 & 0.858 & 0.106\\
Perceptron          & 0.305 & 0.777 & 0.438 & 0.121 & 0.837 & 0.212\\
Joint Unconstrained & 0.220 & 0.820 & 0.347 & 0.141 & 0.758 & 0.237\\
Joint Constrained   & 0.220 & 0.820 & 0.347 & 0.143 & 0.587 & 0.230\\
\end{tabular}
\end{center}
\end{table}

The Joint Unconstrained model scored the highest evaluation values for the Argument extraction task in comparison to the Joint Constrained model which we expected to score better. the Perceptron algorithm scored higher than both Joint algorithms in the Trigger Extraction task due to the developed features performs better in the per-task model boosting Precision metrics, however the Joint algorithms had better Recall metrics. On the contrary, Naive Bayes achieved the highest Recall score on the same task. Based on these conclusions we know that Precision plays a large part in determining the best prediction algorithm on a extraction task. On the other hand, The Joint Constrained Model scored the best Precision metric in the Argument extraction task but also the worst Recall Metric which would suggest why the model's F1 score did not come out as the best.

All in all, we have seen both expected and unexpected outcomes where The Naive Bayes scoring model reached the lowest metrics and the Joint Constrained algorithm scored higher than both the previous and Perceptron algorithms as supposed. However, we had not anticipated the Joint Unconstrained model as better predictioner than the Constrained Model in the Argument extraction task.

\section*{Problem 6: Error Analysis}

The best model we have developed so far is the Perceptron Algorithm. Table \ref{table:Error1} and Table \ref{table:Error2} indicate frequencies for triggers and arguments where our model misclassified within the dev set. 

\begin{table}[!htbp]
    \caption{Most frequent errors given by the Perceptron algorithm for Trigger Extraction. (B = Binding, GE = Gene Expression, L = Localisation, -R = Negative Regulation, N = None, P = Phosphorylation, +P = Positive Regulation, PC = Protein Catabolism, R = Regulation, T = Transcription)}
    \label{table:Error1}
\begin{center}
\begin{tabular}{c c c c c c c c c c c}
\multicolumn{1}{c}{} & \multicolumn{10}{c}{\bf Trigger mislabelled as}
\\ \hline \\
\textbf{Gold}  & \textbf{B} & \textbf{GE} & \textbf{L} & \textbf{-R} & \textbf{N} & \textbf{P} & \textbf{+R} & \textbf{PC} & \textbf{R} & \textbf{T}\\
\textbf{B}  & N/A & 233 & N/A & 257 & 1207 & N/A & 63 & N/A & 72 & 137\\
%\textbf{GE} & 145 & N/A & 81 & 230 & 1122 & N/A & 378 & N/A & 69 & 942\\
% \textbf{L}  & N/A & 521 & N/A & 124 & 625 & N/A & 150 & 65 & 69 & 72\\
\textbf{-R} & 64 & 76 & N/A & N/A & 1380 & N/A & 815 & 64 & 163 & 157\\
\textbf{N}  & 18385 & 12811 & 1612 & 26920 & N/A & 1270 & 70043 & 1032 & 26754 & 17711\\
% \textbf{P}  & N/A & 61 & N/A & N/A & N/A & N/A & 87 & N/A & N/A & N/A\\
\textbf{+R} & 157 & 67 & N/A & 538 & 6319 & N/A & N/A & N/A & 778 & 531\\
% \textbf{PC} & N/A & N/A & N/A & N/A & 72 & N/A & 86 & N/A & N/A & N/A\\
\textbf{R}  & 164 & N/A & N/A & N/A & 869 & N/A & 2285 & 95 & N/A & 60\\
% \textbf{T}  & N/A & 1263 & N/A & N/A & 579 & N/A & 170 & N/A & N/A & N/A\\

\end{tabular}
\end{center}
\end{table}

\begin{table}[!htbp]
\caption{Most frequent errors given by the Perceptron algorithm for Argument Extraction.}
\label{table:Error2}
\begin{center}
\begin{tabular}{c c c c}
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\bf Argument mislabelled as}
\\ \hline \\
\textbf{Gold}  & \textbf{Cause} & \textbf{Theme} & \textbf{None}\\
\textbf{None}  & 32728 & 161881 & N/A\\
\textbf{Theme} & 785 & N/A & 2896\\
\textbf{Cause} & N/A & 884 & 826\\

\end{tabular}
\end{center}
\end{table}

These triggers were mislabelled as Regulation and Positive Regulation when they should have been labelled as None (triggers in bold font):

%  \textit{IL-10 pretreatment prevented \textbf{LPS-induced} decreases in IkappaB-alpha protein levels and attenuated NF-kappaB DNA binding.}

\textit{The glucocorticoid receptor was able to further enhance IL-4 -induced gene transcription through the \textbf{action} of Stat6.}

\textit{This study tested the hypothesis that anti-LPS Abs neutralize endotoxin by \textbf{blocking} cellular uptake through mCD14.}
 
These arguments were mislabelled as Theme and Cause when they should have been labelled None:

\textit{The complex on the proximal site ( \textbf{NF kappa B2} ) appears to be composed of p50 and relB.}

\textit{Coactivation by \textbf{OCA-B} : definition of critical regions and synergism with general cofactors.}

By using the Perceptron algorithm for Argument extraction, we find instances where the argument is misclassified because it evaluates both the triggers and the arguments separately whereas a joint model considers the connection between them for prediction and therefore it can reduce this type of errors by analyzing the full context on every iteration. 

Adding more constraints to the joint constrained model will help remedy specific argument errors i.e. a constraint for binding triggers to accept only Theme events to mentions. In addition, some improvements can be done by expanding the work carried in our data analysis work flow (see attached) which would help us to identify better and more specific high impact features for regulation triggers and corresponding theme and cause events which seems some challenging to model.

\section*{Problem 7: Joint Conditional Likelihood}
We are looking to maximize the conditional log-likelihood of the data by using the stochastic gradient descent/ascent method. 

%We need to derive the gradient of the following objective:

%\begin{equation}
%\sum_{(\mathbf{x}_i,c_i)}\log p_{\boldsymbol{\lambda}}(c_i|\mathbf{x}_i)
%\end{equation}

Assuming that $p_{\boldsymbol{\lambda}}(c_i|\mathbf{x}_i)$, takes the following form:

\begin{equation}
exp\bigg(\sum_{k_i}\lambda_{k_i}\phi_{k_i}(\mathbf{x}_i,c_i)\bigg)
\end{equation}

The index $k_i$ represents the different values that $\boldsymbol{\lambda}$ can take with respect to the pair $(\mathbf{x}_i, c_i)$. The objective then takes the following form:

\begin{equation}
\sum_{(\mathbf{x}_i,c_i)}\sum_{k_i}\lambda_{k_i}\phi_{k_i}(\mathbf{x}_i,c_i)
\end{equation}

 Partially differentiating with respect to each component of $\lambda_{k_i}$ will yield a sum of unique feature functions with non-unique arguments $(\mathbf{x}_i, c_i)$. When $c_i$ is chosen for a given $\mathbf{x_i}$ such that all the associated feature functions $\phi_{k_i}$ are 0, then we have reached the optimum for the objective function.
 
 In terms of computation, the naive and prohibitively expensive way of doing this is simply by iterating through every possible pair of $(\mathbf{x}_i, c_i)$ - i.e. the entire training set and deriving all feature functions for those pairs. A more tractable method could be reducing the total number of $c_i$ given $\mathbf{x_i}$  (i.e. by noting that associated candidate argument tokens must exist in the same sentence as the source token). 

\printbibliography
\end{document}